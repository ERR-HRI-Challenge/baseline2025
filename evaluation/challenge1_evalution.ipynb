{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8276745-2f96-4146-b143-dab6df969d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score, accuracy_score, confusion_matrix\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score, accuracy_score, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92284cad-2ffa-4b5e-8155-6619232402d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_path = \"./submissions/y_pred_challenge1.csv\"\n",
    "\n",
    "y_pred_df = pd.read_csv(y_pred_path, low_memory=False)\n",
    "\n",
    "y_true_path = \"./y_true_df_challenge1.csv\"\n",
    "y_true_df = pd.read_csv(y_true_path, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b183e8-dae6-4cc5-a78d-c9176b194d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_error(y_pred_df, y_true_df):\n",
    "    \"\"\"\n",
    "    Compute classification metrics and count of detected error events.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred_df : pd.DataFrame\n",
    "        DataFrame must contain 'task', 'trial', 'start', 'end', 'y_pred_err',\n",
    "    y_pred_df : pd.DataFrame\n",
    "        DataFrame must contain 'task', 'trial', 'error_onset', 'error_offset'.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tp: number of true positives \n",
    "    fp: number of false positives\n",
    "    total_errors: number of errors in total\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure y_true only include trials in pred\n",
    "    evaluation_trials = y_pred_df['trial'].astype(int).unique()\n",
    "    y_true_df = y_true_df.loc[y_true_df['trial'].isin(evaluation_trials)]\n",
    "    \n",
    "    # Look for true positive and false positive \n",
    "    pos_pred = y_pred_df.loc[y_pred_df['y_pred_err'] == 1]\n",
    "    pos_pred['id'] = pos_pred.index\n",
    "    pos_pred['overlap_error'] = 0\n",
    "\n",
    "    # initialize metrics\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    total_errors = len(y_true_df)\n",
    "\n",
    "    # check that the trials contain errors \n",
    "    if len(y_true_df) > 0:\n",
    "        for _, row in y_true_df.iterrows():\n",
    "            task = row['task']\n",
    "            trial = row['trial']\n",
    "            error_onset = row['error_onset'].total_seconds() - 1 # added one second tolerance\n",
    "            error_offset = row['error_offset'].total_seconds() + 1 # added one second tolerance\n",
    "\n",
    "            # check if the predicted error overlaps with actual error \n",
    "            detected_err = pos_pred[(pos_pred['task'] == task) & (pos_pred['trial'] == trial) & \n",
    "                                    ((pos_pred['start'] >= error_onset) & (pos_pred['start'] <= error_offset)) |\n",
    "                                    ((pos_pred['end']   >= error_onset) & (pos_pred['end']   <= error_offset)) |\n",
    "                                    ((pos_pred['start'] <= error_onset) & (pos_pred['end'] >= error_offset))]\n",
    "            pos_pred.loc[pos_pred['id'].isin(detected_err['id'])] = 1\n",
    "            if len(detected_err) > 0: \n",
    "                tp = tp + 1\n",
    "\n",
    "        # prediction is a false positive if it does not overlap with actual error \n",
    "        fp = len(pos_pred.loc[pos_pred['overlap_error'] == 0])\n",
    "        \n",
    "        print(f\"True Positive: {tp} ({tp / total_errors * 100}/%)\")\n",
    "        print(f\"False Positive: {fp}\")\n",
    "\n",
    "    return tp, fp, total_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b634936-8661-4667-8227-3e3d30c81ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_error(y_pred_df, y_true_df):\n",
    "    \"\"\"\n",
    "    Compute classification metrics and count of detected error events.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred_df : pd.DataFrame\n",
    "        DataFrame must contain 'task', 'trial', 'start', 'end', 'y_pred_err',\n",
    "    y_pred_df : pd.DataFrame\n",
    "        DataFrame must contain 'task', 'trial', 'error_onset', 'error_offset'.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tp: number of true positives \n",
    "    fp: number of false positives\n",
    "    total_errors: number of errors in total\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting Error Evaluation\")\n",
    "    \n",
    "    # Make sure y_true only include trials in pred\n",
    "    evaluation_trials = y_pred_df['trial'].astype(int).unique()\n",
    "    y_true_df = y_true_df.loc[y_true_df['trial'].isin(evaluation_trials)]\n",
    "    \n",
    "    # Look for true positive and false positive \n",
    "    pos_pred = y_pred_df.loc[y_pred_df['y_pred_err'] == 1]\n",
    "    pos_pred['id'] = pos_pred.index\n",
    "    pos_pred['overlap_error'] = 0\n",
    "\n",
    "    # initialize metrics\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    total_errors = len(y_true_df)\n",
    "    print(total_errors)\n",
    "\n",
    "    y_true_df['error_onset'] = pd.to_timedelta(y_true_df['error_onset'])\n",
    "    y_true_df['error_offset'] = pd.to_timedelta(y_true_df['error_offset'])\n",
    "              \n",
    "    # check that the trials contain errors \n",
    "    if len(y_true_df) > 0:\n",
    "        for _, row in y_true_df.iterrows():\n",
    "            task = row['task']\n",
    "            trial = row['trial']\n",
    "\n",
    "            error_onset = row['error_onset'].total_seconds() - 1 # added one second tolerance\n",
    "            error_offset = row['error_offset'].total_seconds() + 1 # added one second tolerance\n",
    "\n",
    "            # check if the predicted error overlaps with actual error \n",
    "            detected_err = pos_pred[(pos_pred['task'] == task) & (pos_pred['trial'] == trial) & \n",
    "                                    ((pos_pred['start'] >= error_onset) & (pos_pred['start'] <= error_offset)) |\n",
    "                                    ((pos_pred['end']   >= error_onset) & (pos_pred['end']   <= error_offset)) |\n",
    "                                    ((pos_pred['start'] <= error_onset) & (pos_pred['end'] >= error_offset))]\n",
    "            pos_pred.loc[pos_pred['id'].isin(detected_err['id'])] = 1\n",
    "            if len(detected_err) > 0: \n",
    "                tp = tp + 1\n",
    "\n",
    "        # prediction is a false positive if it does not overlap with actual error \n",
    "        fp = len(pos_pred.loc[pos_pred['overlap_error'] == 0])\n",
    "        \n",
    "        print(f\"True Positive: {tp} ({tp / total_errors * 100}/%)\")\n",
    "        print(f\"False Positive: {fp}\")\n",
    "\n",
    "        fn = total_errors - tp\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = 2 * precision * recall / (precision + recall) \n",
    "        print(f\"F1: {f1}\")\n",
    "\n",
    "    return tp, fp, total_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba802841-3cc5-415f-9cb7-000eaf815e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df = y_pred_df.rename(columns={'y_pred_reaction': 'y_pred_err'})\n",
    "\n",
    "tp, fp, total_error = evaluate_error(y_pred_df, y_true_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
