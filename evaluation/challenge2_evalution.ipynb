{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8276745-2f96-4146-b143-dab6df969d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score, accuracy_score, confusion_matrix\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score, accuracy_score, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92284cad-2ffa-4b5e-8155-6619232402d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_path = \"./submissions/y_pred_challenge2.csv\"\n",
    "\n",
    "y_pred_df = pd.read_csv(y_pred_path, low_memory=False)\n",
    "\n",
    "y_true_path = \"./y_true_df_challenge2.csv\"\n",
    "y_true_df = pd.read_csv(y_true_path, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b634936-8661-4667-8227-3e3d30c81ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reaction(y_pred_df, y_true_df):\n",
    "    \"\"\"\n",
    "    Compute classification metrics and count of detected error events.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred_df : pd.DataFrame\n",
    "        DataFrame must contain 'task', 'trial', 'start', 'end', 'y_pred_reaction',\n",
    "    y_pred_df : pd.DataFrame\n",
    "        DataFrame must contain 'task', 'trial', 'reaction_onset', 'reaction_offset'.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tp: number of true positives \n",
    "    fp: number of false positives\n",
    "    total_reactions: number of reactions in total\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure y_true only include trials in pred\n",
    "    evaluation_trials = y_pred_df['trial'].astype(int).unique()\n",
    "    y_true_df = y_true_df.loc[y_true_df['trial'].isin(evaluation_trials)]\n",
    "    \n",
    "    # Look for true positive and false positive \n",
    "    pos_pred = y_pred_df.loc[y_pred_df['y_pred_reaction'] == 1]\n",
    "    pos_pred['id'] = pos_pred.index\n",
    "    pos_pred['overlap_reaction'] = 0\n",
    "\n",
    "    # initialize metrics\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    total_reactions = len(y_true_df)\n",
    "\n",
    "    y_true_df['reaction_onset'] = pd.to_timedelta(y_true_df['reaction_onset'])\n",
    "    y_true_df['reaction_offset'] = pd.to_timedelta(y_true_df['reaction_offset'])\n",
    "    \n",
    "    # check that the trials contain reactions \n",
    "    if len(y_true_df) > 0:\n",
    "        for _, row in y_true_df.iterrows():\n",
    "            task = row['task']\n",
    "            trial = row['trial']\n",
    "            reaction_onset = row['reaction_onset'].total_seconds() - 1 # added one second tolerance\n",
    "            reaction_offset = row['reaction_offset'].total_seconds() + 1 # added one second tolerance\n",
    "\n",
    "            # check if the predicted reaction overlaps with actual reaction \n",
    "            detected_err = pos_pred[(pos_pred['task'] == task) & (pos_pred['trial'] == trial) & \n",
    "                                    ((pos_pred['start'] >= reaction_onset) & (pos_pred['start'] <= reaction_offset)) |\n",
    "                                    ((pos_pred['end']   >= reaction_onset) & (pos_pred['end']   <= reaction_offset)) |\n",
    "                                    ((pos_pred['start'] <= reaction_onset) & (pos_pred['end'] >= reaction_offset))]\n",
    "            pos_pred.loc[pos_pred['id'].isin(detected_err['id']), 'overlap_reaction'] = 1\n",
    "            if len(detected_err) > 0: \n",
    "                tp = tp + 1\n",
    "\n",
    "        # prediction is a false positive if it does not overlap with actual reaction\n",
    "        fp = len(pos_pred.loc[pos_pred['overlap_reaction'] == 0])\n",
    "        \n",
    "        print(f\"True Positive: {tp} ({tp / total_reactions * 100}/%)\")\n",
    "        print(f\"False Positive: {fp}\")\n",
    "\n",
    "        fn = total_reactions - tp\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = 2 * precision * recall / (precision + recall) \n",
    "        print(f\"F1: {f1}\")\n",
    "\n",
    "    return tp, fp, total_reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba802841-3cc5-415f-9cb7-000eaf815e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, total_error = evaluate_reaction(y_pred_df, y_true_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53889cb1-a522-40e1-9a45-a384ae8a8978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
