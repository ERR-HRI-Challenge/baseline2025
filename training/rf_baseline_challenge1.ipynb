{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8276745-2f96-4146-b143-dab6df969d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score, accuracy_score, confusion_matrix\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score, accuracy_score, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92284cad-2ffa-4b5e-8155-6619232402d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"./rf_train_window_size_3.0_stride_0.5.csv\"\n",
    "test_data_path = \"./rf_test_window_size_3.0_stride_0.5.csv\"\n",
    "\n",
    "train_data = pd.read_csv(train_data_path, low_memory=False)\n",
    "test_data = pd.read_csv(test_data_path, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "673de34b-c4ba-4173-b246-0f0f12029383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of windows: 84116\n",
      "Number of features: 2329\n",
      "Positive Rate: 11.345047315611774%\n"
     ]
    }
   ],
   "source": [
    "n_windows = len(train_data)\n",
    "print(f\"Number of windows: {n_windows}\")\n",
    "\n",
    "n_features = len(train_data.columns) \n",
    "print(f\"Number of features: {n_features}\")\n",
    "\n",
    "positive_rate = (train_data['robot_error'] == 1).sum() / len(train_data['robot_error']) * 100\n",
    "print(f\"Positive Rate: {positive_rate}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b29ba1e-0845-46c2-933a-737cd0855076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Task: medical\n",
      "\n",
      "  Task: trip\n",
      "\n",
      "  Task: police\n",
      "\n",
      "  Task: survival\n",
      "\n",
      "  Task: discussion\n"
     ]
    }
   ],
   "source": [
    "# Load Test Data\n",
    "# Naming Pattern for Label Files\n",
    "LABEL_FILE_PATTERNS = {\n",
    "    \"robot_errors\": \"challenge1_robot_error_labels_{task}_test.csv\",\n",
    "    \"human_reactions_ch1\": \"challenge1_user_reaction_labels_{task}_test.csv\",\n",
    "    \"human_reactions_ch2\": \"challenge2_user_reaction_labels_{task}_test.csv\"\n",
    "}\n",
    "\n",
    "def fix_timestamp_format(ts):\n",
    "    if isinstance(ts, str):\n",
    "        parts = ts.split(':')\n",
    "        if len(parts) == 4:\n",
    "            return ':'.join(parts[:3]) + '.' + parts[3]\n",
    "    return ts\n",
    "\n",
    "\"\"\"Load all label files for a specific task and preprocess timestamps\"\"\"\n",
    "# Base Path to Data\n",
    "BASE_PATH = \"./ACM-MM-ERR-HRI-2025-Dataset\"\n",
    "\n",
    "def load_and_preprocess_labels(task):\n",
    "    \"\"\"Load all label files for a specific task and preprocess timestamps\"\"\"\n",
    "    labels = {}    \n",
    "    \n",
    "    try:\n",
    "        # Load robot error labels\n",
    "        robot_errors = pd.read_csv(\n",
    "            f\"{BASE_PATH}/labels_test/challenge1_test/{LABEL_FILE_PATTERNS['robot_errors'].format(task=task)}\"\n",
    "        )\n",
    "        robot_errors['error_onset'] = robot_errors['error_onset'].apply(fix_timestamp_format)\n",
    "        robot_errors['error_onset'] = pd.to_timedelta(robot_errors['error_onset'])\n",
    "        robot_errors['error_offset'] = robot_errors['error_offset'].apply(fix_timestamp_format)\n",
    "        robot_errors['error_offset'] = pd.to_timedelta(robot_errors['error_offset'])\n",
    "        robot_errors['trial'] = robot_errors['trial_name'].apply(lambda s: s.split('-', 1)[0])\n",
    "        labels['robot_errors'] = robot_errors        \n",
    "        \n",
    "        # Load human reaction labels\n",
    "        human_reactions_ch1 = pd.read_csv(\n",
    "            f\"{BASE_PATH}/labels_test/challenge1_test/{LABEL_FILE_PATTERNS['human_reactions_ch1'].format(task=task)}\"\n",
    "        )\n",
    "        human_reactions_ch1['reaction_onset'] = human_reactions_ch1['reaction_onset'].apply(fix_timestamp_format)\n",
    "        human_reactions_ch1['reaction_onset'] = pd.to_timedelta(human_reactions_ch1['reaction_onset'])\n",
    "        human_reactions_ch1['reaction_offset'] = human_reactions_ch1['reaction_offset'].apply(fix_timestamp_format)\n",
    "        human_reactions_ch1['reaction_offset'] = pd.to_timedelta(human_reactions_ch1['reaction_offset'])\n",
    "        human_reactions_ch1['trial'] = human_reactions_ch1['trial_name'].apply(lambda s: s.split('-', 1)[0])\n",
    "        labels['human_reactions_ch1'] = human_reactions_ch1        \n",
    "        \n",
    "        human_reactions_ch2 = pd.read_csv(\n",
    "            f\"{BASE_PATH}/labels_test/challenge2_test/{LABEL_FILE_PATTERNS['human_reactions_ch2'].format(task=task)}\"\n",
    "        )\n",
    "        human_reactions_ch2['reaction_onset'] = human_reactions_ch2['reaction_onset'].apply(fix_timestamp_format)\n",
    "        human_reactions_ch2['reaction_onset'] = pd.to_timedelta(human_reactions_ch2['reaction_onset'])\n",
    "        human_reactions_ch2['reaction_offset'] = human_reactions_ch2['reaction_offset'].apply(fix_timestamp_format)\n",
    "        human_reactions_ch2['reaction_offset'] = pd.to_timedelta(human_reactions_ch2['reaction_offset'])\n",
    "        human_reactions_ch2['trial'] = human_reactions_ch2['trial_name'].apply(lambda s: s.split('-', 1)[0])\n",
    "        labels['human_reactions_ch2'] = human_reactions_ch2    \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading label files for task {task}: {str(e)}\")\n",
    "        return None    \n",
    "    \n",
    "    return labels, robot_errors, human_reactions_ch1, human_reactions_ch2\n",
    "\n",
    "all_robot_errors = pd.DataFrame()\n",
    "all_human_reactions_ch1 = pd.DataFrame()\n",
    "all_human_reactions_ch2 = pd.DataFrame()\n",
    "\n",
    "tasks = test_data['task'].unique()\n",
    "for task in tasks:\n",
    "    print(f\"\\n  Task: {task}\")            \n",
    "    \n",
    "    # Load label files for this task\n",
    "    labels, robot_errors, human_reactions_ch1, human_reactions_ch2 = load_and_preprocess_labels(task)\n",
    "    robot_errors['task'] = task\n",
    "    human_reactions_ch1['task'] = task\n",
    "    human_reactions_ch2['task'] = task\n",
    "    all_robot_errors = pd.concat([all_robot_errors, robot_errors], ignore_index=True)\n",
    "    all_human_reactions_ch1 = pd.concat([all_human_reactions_ch1, human_reactions_ch1], ignore_index=True)\n",
    "    all_human_reactions_ch2 = pd.concat([all_human_reactions_ch2, human_reactions_ch2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5b183e8-dae6-4cc5-a78d-c9176b194d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_error(y_pred_df, y_true_df):\n",
    "    \"\"\"\n",
    "    Compute classification metrics and count of detected error events.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred_df : pd.DataFrame\n",
    "        DataFrame must contain 'task', 'trial', 'start', 'end', 'y_pred_err',\n",
    "    y_pred_df : pd.DataFrame\n",
    "        DataFrame must contain 'task', 'trial', 'error_onset', 'error_offset'.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tp: number of true positives \n",
    "    fp: number of false positives\n",
    "    total_errors: number of errors in total\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure y_true only include trials in pred\n",
    "    evaluation_trials = y_pred_df['trial'].astype(str).unique()\n",
    "    y_true_df = y_true_df.loc[y_true_df['trial'].isin(evaluation_trials)]\n",
    "    \n",
    "    # Look for true positive and false positive \n",
    "    pos_pred = y_pred_df.loc[y_pred_df['y_pred_err'] == 1]\n",
    "    pos_pred['id'] = pos_pred.index\n",
    "    pos_pred['overlap_error'] = 0\n",
    "\n",
    "    # initialize metrics\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    total_errors = len(y_true_df)\n",
    "\n",
    "    # check that the trials contain errors \n",
    "    if len(y_true_df) > 0:\n",
    "        for _, row in y_true_df.iterrows():\n",
    "            task = row['task']\n",
    "            trial = row['trial']\n",
    "            error_onset = row['error_onset'].total_seconds() - 1 # added one second tolerance\n",
    "            error_offset = row['error_offset'].total_seconds() + 1 # added one second tolerance\n",
    "\n",
    "            # check if the predicted error overlaps with actual error \n",
    "            detected_err = pos_pred[(pos_pred['task'] == task) & (pos_pred['trial'] == trial) & \n",
    "                                    ((pos_pred['start'] >= error_onset) & (pos_pred['start'] <= error_offset)) |\n",
    "                                    ((pos_pred['end']   >= error_onset) & (pos_pred['end']   <= error_offset)) |\n",
    "                                    ((pos_pred['start'] <= error_onset) & (pos_pred['end'] >= error_offset))]\n",
    "            pos_pred.loc[pos_pred['id'].isin(detected_err['id'])] = 1\n",
    "            if len(detected_err) > 0: \n",
    "                tp = tp + 1\n",
    "\n",
    "        # prediction is a false positive if it does not overlap with actual error \n",
    "        fp = len(pos_pred.loc[pos_pred['overlap_error'] == 0])\n",
    "        \n",
    "        print(f\"True Positive: {tp} ({tp / total_errors * 100}/%)\")\n",
    "        print(f\"False Positive: {fp}\")\n",
    "\n",
    "        fn = total_errors - tp\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = 2 * precision * recall / (precision + recall) \n",
    "        print(f\"F1: {f1}\")\n",
    "\n",
    "    return tp, fp, total_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9803b7a-f8d4-4b60-8eaf-593b18160e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shiyecao/Desktop/ACM-MM/err-hri-2025-baseline-model/venv/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  • Training robot_error model… done.\n",
      "Top 10 feature importances:\n",
      "transcript_dim_365_max             0.003311\n",
      "transcript_dim_365_std             0.002952\n",
      "transcript_dim_301_min             0.002871\n",
      "transcript_dim_221_min             0.002612\n",
      "audio_logRelF0-H1-H2_sma3nz_max    0.002474\n",
      "transcript_dim_301_std             0.002361\n",
      "transcript_dim_430_max             0.002185\n",
      "transcript_dim_365_min             0.002097\n",
      "transcript_dim_197_max             0.002090\n",
      "transcript_dim_221_std             0.002030\n",
      "dtype: float64\n",
      "threshold: 0.6\n",
      "AUC: 0.589, F1: 0.493, Acc: 0.825, TPR: 0.057, FPR: 0.051, TNR: 0.949, FNR: 0.943, \n",
      "True Positive: 34 (39.53488372093023/%)\n",
      "False Positive: 163\n",
      "F1: 0.24028268551236745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9g/5yjdy44d0h55vk_qgd4zc_vm0000gn/T/ipykernel_38982/2381195859.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pos_pred['id'] = pos_pred.index\n",
      "/var/folders/9g/5yjdy44d0h55vk_qgd4zc_vm0000gn/T/ipykernel_38982/2381195859.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pos_pred['overlap_error'] = 0\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# split\n",
    "df_train = train_data.copy()\n",
    "df_val   = test_data.copy()\n",
    "\n",
    "X_train = df_train.drop(columns=[\n",
    "    'start','end','robot_error','reaction_ch1','reaction_ch2',\n",
    "    'reaction_type','system','task','trial'\n",
    "])\n",
    "X_val   = df_val.drop(columns=[\n",
    "    'start','end','robot_error','reaction_ch1','reaction_ch2',\n",
    "    'reaction_type','system','task','trial'\n",
    "])\n",
    "\n",
    "y_err_train = df_train['robot_error']\n",
    "y_err_val   = df_val['robot_error']\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "\n",
    "# with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train_imputed, y_err_train)\n",
    "X_train_sm = pd.DataFrame(X_train_sm, columns=X_train.columns)\n",
    "\n",
    "# instantiate\n",
    "# clf_err = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "clf_err = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"  • Training robot_error model…\", end=\"\", flush=True)\n",
    "# clf_err.fit(X_train, y_err_train)\n",
    "clf_err.fit(X_train_sm, y_train_sm)\n",
    "print(\" done.\")\n",
    "\n",
    "# feature importances\n",
    "feat_imp = pd.Series(\n",
    "    clf_err.feature_importances_, \n",
    "    index=X_train.columns\n",
    ").sort_values(ascending=False)\n",
    "print(\"Top 10 feature importances:\")\n",
    "print(feat_imp.head(10))\n",
    "\n",
    "# predict\n",
    "probs_err = clf_err.predict_proba(X_val)[:, 1]\n",
    "threshold = 0.6\n",
    "print(f'threshold: {threshold}')\n",
    "pred_err = (probs_err >= threshold).astype(int)\n",
    "\n",
    "# compute error-model metrics\n",
    "f1_e = f1_score(y_err_val, pred_err, average='macro')\n",
    "acc_e  = accuracy_score(y_err_val, pred_err)\n",
    "if sum(y_err_val) > 0: \n",
    "    tn, fp, fn, tp = confusion_matrix(y_err_val, pred_err).ravel()\n",
    "else: \n",
    "    tn, fp, fn, tp = 0, 0, 0, 0\n",
    "\n",
    "tpr_e = tp / (tp + fn) \n",
    "fpr_e = fp / (fp + tn) \n",
    "tnr_e = tn / (tn + fp) \n",
    "fnr_e = fn / (fn + tp) \n",
    "auc_e = roc_auc_score(y_err_val, probs_err)\n",
    "\n",
    "# print fold metrics\n",
    "print(\n",
    "    f\"AUC: {auc_e:.3f}, \" \n",
    "    f\"F1: {f1_e:.3f}, Acc: {acc_e:.3f}, \"\n",
    "    f\"TPR: {tpr_e:.3f}, FPR: {fpr_e:.3f}, \"\n",
    "    f\"TNR: {tnr_e:.3f}, FNR: {fnr_e:.3f}, \"\n",
    ")\n",
    "\n",
    "df_val['y_pred_err'] = pred_err\n",
    "val_trials = df_val['trial'].astype(str).unique()\n",
    "\n",
    "y_pred_df = df_val[['task', 'trial', 'start', 'end', 'y_pred_err']]\n",
    "y_true_df = all_robot_errors[['task', 'trial', 'error_onset', 'error_offset']].loc[all_robot_errors['trial'].isin(val_trials)]\n",
    "\n",
    "y_true_df.to_csv('y_true_df.csv', index=False)\n",
    "\n",
    "tp, fp, total_error = evaluate_error(y_pred_df, y_true_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
